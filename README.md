# Conversational RAG with PDF Uploads and Chat History

This project implements a Retrieval-Augmented Generation (RAG) system that allows users to upload PDF documents and engage in a conversational question-answering session about their content.  The system utilizes Streamlit for the user interface, Langchain for orchestrating the RAG pipeline, and Groq for fast LLM inference.  Chat history is maintained to provide context-aware responses throughout the conversation.

## Features

*   **PDF Upload:**  Users can upload one or more PDF files directly through the Streamlit interface.
*   **Conversational Q&A:** Ask questions about the content of the uploaded PDFs and receive answers generated by the LLM.
*   **Chat History:** The system remembers previous interactions in the conversation, allowing for follow-up questions and more contextually relevant answers.
*   **Fast Inference:** Leverages Groq for incredibly fast LLM responses.
*   **RAG Pipeline:** Uses a sophisticated RAG pipeline to retrieve relevant information from the documents before generating answers.

## Technologies Used

*   **Streamlit:** For creating the interactive web application.
*   **Langchain:**  A framework for building LLM-powered applications, used here to implement the RAG pipeline.
*   **Groq:**  For accessing a large language model (LLM) with high speed and low latency.  Specifically, the `qwen-2.5-32b` model is used.
*   **Hugging Face Embeddings:**  For generating embeddings of the PDF document chunks.  Uses the `all-MiniLM-L6-v2` model.
*   **Chroma:**  A vector database used to store and efficiently retrieve document embeddings.
*   **PyPDFLoader:**  For loading content from PDF files.
*   **Python:** The core programming language.
